{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('etsynecklaces10000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9967"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf=df[df['cur']=='USD']\n",
    "len(usdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleanup(text):\n",
    "    text = list(set([txt.strip() for txt in text]))\n",
    "    text = ' '.join(text).strip()\n",
    "    linkregex = r\"((([A-Za-z]{3,9}:(?:\\/\\/)?)(?:[-;:&=\\+\\$,\\w]+@)?[A-Za-z0-9.-]+|(?:www.|[-;:&=\\+\\$,\\w]+@)[A-Za-z0-9.-]+)((?:\\/[\\+~%\\/.\\w\\-_]*)?\\??(?:[-\\+=&;%@.\\w_]*)#?(?:[\\w]*))?)\"\n",
    "    text = re.sub(linkregex, '', text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\d\\s]\", '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    s=set(stopwords.words('english'))\n",
    "    text = ' '.join([x for x in text.split() if not x in s])\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = ' '.join([x for x in text.split() if not bool(re.search(r'\\d', x))])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "usdf['price'] = pd.to_numeric(usdf['price'].apply(lambda x: x.replace(',','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(usdf[usdf['price']>500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "usdf['description']=usdf['description'].apply(lambda x: text_cleanup(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "usdf['desc']=usdf['description'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def stem_words(text):\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    text = ' '.join([stemmer.stem(x) for x in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def viterbi_segment(text):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        #(log(1/total)-max_word_len-1)*(j-i) \n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "                        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words, probs[-1]\n",
    "\n",
    "def word_prob(word): return dictionary_US[word] / total\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "dictionary_US = Counter(words(open('/home/eli/Downloads/big.txt').read()))\n",
    "max_word_length = max(map(len, dictionary_US))\n",
    "total = float(sum(dictionary_US.values()))\n",
    "\n",
    "with open('/usr/share/dict/american-english') as fdict:\n",
    "    US_dict = fdict.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "US_dict = [x.strip() for x in US_dict] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "usdf['adesc'] = usdf['desc'].apply(lambda x: remove_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    usdf['adesc'].values, usdf['price'].values, test_size=0.25, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"/home/eli/code/insight/etsy/etsyitems/nlp\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = \"/home/eli/code/insight/etsy/etsyitems/nlp\"\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6192,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2065,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [text.split() for text in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [text.split() for text in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTOR SPACE MODEL #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70710678, 0.70710678])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "def normvec(vec):\n",
    "    normv = normalize(vec[:,np.newaxis], axis=0).ravel()\n",
    "    return normv\n",
    "\n",
    "normvec(np.array([1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_normalize(vec):\n",
    "    with np.errstate(divide='raise'):\n",
    "        try:\n",
    "            res = vec/np.sqrt(np.sum(np.square(vec)))\n",
    "        except FloatingPointError: \n",
    "            print(vec)\n",
    "            res=None\n",
    "    return res\n",
    "\n",
    "assert(np.array_equal(vec_normalize([1,0]),[1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-29 14:17:45,052 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-01-29 14:17:46,018 : INFO : built Dictionary(21051 unique tokens: ['actual', 'appear', 'barn', 'beautifully', 'beige']...) from 6192 documents (total 800541 corpus positions)\n",
      "2019-01-29 14:17:46,019 : INFO : saving Dictionary object under /home/eli/code/insight/etsy/etsyitems/nlp/necklaces.dict, separately None\n",
      "2019-01-29 14:17:46,027 : INFO : saved /home/eli/code/insight/etsy/etsyitems/nlp/necklaces.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(21051 unique tokens: ['actual', 'appear', 'barn', 'beautifully', 'beige']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(train_texts)\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'necklaces.dict'))  # store the dictionary, for future reference\n",
    "#dictionary = corpora.dictionary.Dictionary.load(os.path.join(TEMP_FOLDER, 'necklaces.dict'))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-29 14:18:20,665 : INFO : storing corpus in Matrix Market format to /home/eli/code/insight/etsy/etsyitems/nlp/necklaces.mm\n",
      "2019-01-29 14:18:20,666 : INFO : saving sparse matrix to /home/eli/code/insight/etsy/etsyitems/nlp/necklaces.mm\n",
      "2019-01-29 14:18:20,667 : INFO : PROGRESS: saving document #0\n",
      "2019-01-29 14:18:20,823 : INFO : PROGRESS: saving document #1000\n",
      "2019-01-29 14:18:20,970 : INFO : PROGRESS: saving document #2000\n",
      "2019-01-29 14:18:21,132 : INFO : PROGRESS: saving document #3000\n",
      "2019-01-29 14:18:21,285 : INFO : PROGRESS: saving document #4000\n",
      "2019-01-29 14:18:21,435 : INFO : PROGRESS: saving document #5000\n",
      "2019-01-29 14:18:21,591 : INFO : PROGRESS: saving document #6000\n",
      "2019-01-29 14:18:21,625 : INFO : saved 6192x21051 matrix, density=0.431% (562122/130347792)\n",
      "2019-01-29 14:18:21,627 : INFO : saving MmCorpus index to /home/eli/code/insight/etsy/etsyitems/nlp/necklaces.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'necklaces.mm'), corpus)\n",
    "#corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'necklaces.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-29 14:18:38,066 : INFO : collecting document frequencies\n",
      "2019-01-29 14:18:38,067 : INFO : PROGRESS: processing document #0\n",
      "2019-01-29 14:18:38,225 : INFO : calculating IDF weights for 6192 documents and 21050 features (562122 matrix non-zeros)\n",
      "2019-01-29 14:18:38,293 : INFO : saving TfidfModel object under /home/eli/code/insight/etsy/etsyitems/nlp/necklacesmodel.tfidf, separately None\n",
      "2019-01-29 14:18:38,393 : INFO : saved /home/eli/code/insight/etsy/etsyitems/nlp/necklacesmodel.tfidf\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "tfidf.save(os.path.join(TEMP_FOLDER, 'necklacesmodel.tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-29 14:18:54,185 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2019-01-29 14:18:57,498 : INFO : creating matrix with 6192 documents and 21051 features\n",
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2019-01-29 14:19:03,082 : INFO : saving MatrixSimilarity object under /home/eli/code/insight/etsy/etsyitems/nlp/necklacestfidfsim.index, separately None\n",
      "2019-01-29 14:19:03,082 : INFO : storing np array 'index' to /home/eli/code/insight/etsy/etsyitems/nlp/necklacestfidfsim.index.index.npy\n",
      "2019-01-29 14:19:03,522 : INFO : saved /home/eli/code/insight/etsy/etsyitems/nlp/necklacestfidfsim.index\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus]) # transform corpus to Tfidf space and index it\n",
    "index.save(os.path.join(TEMP_FOLDER, 'necklacestfidfsim.index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-29 14:20:48,851 : INFO : storing corpus in Matrix Market format to /home/eli/code/insight/etsy/etsyitems/nlp/necklaces-test.mm\n",
      "2019-01-29 14:20:48,852 : INFO : saving sparse matrix to /home/eli/code/insight/etsy/etsyitems/nlp/necklaces-test.mm\n",
      "2019-01-29 14:20:48,852 : INFO : PROGRESS: saving document #0\n",
      "2019-01-29 14:20:49,003 : INFO : PROGRESS: saving document #1000\n",
      "2019-01-29 14:20:49,140 : INFO : PROGRESS: saving document #2000\n",
      "2019-01-29 14:20:49,149 : INFO : saved 2065x21050 matrix, density=0.421% (183062/43468250)\n",
      "2019-01-29 14:20:49,151 : INFO : saving MmCorpus index to /home/eli/code/insight/etsy/etsyitems/nlp/necklaces-test.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus_test = [dictionary.doc2bow(text) for text in test_texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'necklaces-test.mm'), corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtraintfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtesttfidf = tfidf[corpus_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def read_corpus(texts, tokens_only=False):\n",
    "    for i, text in enumerate(texts):\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(' '.join(text))\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(' '.join(text)), [i])\n",
    "        \n",
    "\n",
    "train_corpus = list(read_corpus(train_texts))\n",
    "test_corpus = list(read_corpus(test_texts, tokens_only=True))\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save(os.path.join(TEMP_FOLDER, 'necklaces.d2v'))\n",
    "\n",
    "%time X_tr = np.array([model.infer_vector(train_corpus[i].words, steps=40, alpha=0.025) for i in range(len(train_corpus))])\n",
    "\n",
    "%time X_tst = np.array([model.infer_vector(test_corpus[i], steps=40, alpha=0.025) for i in range(len(test_corpus))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting and hyperparameter tuning #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_r2(y_pred, y_act):\n",
    "    assert(len(y_pred)==len(y_act))\n",
    "    resid = y_pred-y_act\n",
    "    ssresid = np.sum(np.square(resid))\n",
    "    sstot = np.sum(np.square(y_act-np.mean(y_act)))\n",
    "    r2 = 1-ssresid/sstot\n",
    "    return r2\n",
    "\n",
    "def rmse(y_pred, y_act):\n",
    "    assert(len(y_pred)==len(y_act))\n",
    "    return np.sqrt(np.sum(np.square(y_pred-y_act))/len(y_act))\n",
    "\n",
    "def mape(y_pred, y_act):\n",
    "    assert(len(y_pred)==len(y_act))\n",
    "    return 100*np.sum(np.abs((y_pred-y_act)/y_act))/len(y_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(k, X, y_train, weighting=False):\n",
    "    y_pred =  np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        sims = sorted(enumerate(index[X[i]]), key=lambda item: -item[1])[:k]\n",
    "        if weighting:\n",
    "            weights = np.square(normvec(np.array([v for (k,v) in sims])))\n",
    "            if np.array_equal(weights,np.zeros(len(weights))):\n",
    "                weights=None\n",
    "            y_pred[i] = np.average([y_train[k] for (k,v) in sims], weights=weights)\n",
    "        else:\n",
    "            y_pred[i] = np.average([y_train[k] for (k,v) in sims])\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparamcvknn(klist, X, y, y_train):\n",
    "    results = []\n",
    "    for k in klist:\n",
    "        for weighting in [True]:\n",
    "            y_pred = knn_predict(k, X, y_train, weighting)\n",
    "            results.append((score_r2(y_pred, y),[k,weighting]))\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 41s, sys: 164 ms, total: 7min 41s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "klist = np.linspace(10,11,num=1,dtype=int)\n",
    "res2  = hyperparamcvknn(klist, Xtraintfidf, y_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.768083584597864, [10, True])]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(nan, [10, True]), (0.3118361202556046, [10, False])]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49881934366911485\n"
     ]
    }
   ],
   "source": [
    "def score_testdata(X_test,y_test,y_train,params):\n",
    "    y_pred = knn_predict(params['k'],X_test,y_train,params['weighting'])\n",
    "    return score_r2(y_pred,y_test)\n",
    "                   \n",
    "params = {'k': 10, 'weighting': True}    \n",
    "print(score_testdata(Xtesttfidf, y_test,y_train,params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287.5711151737237\n"
     ]
    }
   ],
   "source": [
    "def rmse_testdata(X_test,y_test,y_train,params):\n",
    "    y_pred = knn_predict(params['k'],X_test,y_train,params['weighting'])\n",
    "    return rmse(y_pred,y_test)\n",
    "                   \n",
    "params = {'k': 10, 'weighting': True}    \n",
    "print(rmse_testdata(Xtesttfidf, y_test,y_train,params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.07408798434541\n"
     ]
    }
   ],
   "source": [
    "def mape_testdata(X_test,y_test,y_train,params):\n",
    "    y_pred = knn_predict(params['k'],X_test,y_train,params['weighting'])\n",
    "    return mape(y_pred,y_test)\n",
    "                   \n",
    "params = {'k': 10, 'weighting': True}    \n",
    "print(mape_testdata(Xtesttfidf, y_test,y_train,params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from time import time\n",
    "# Dimensionality reduction for tfidf\n",
    "def reduce_dim_by_svd(X, ncomp):\n",
    "    t0 = time()\n",
    "    svd = TruncatedSVD(ncomp)\n",
    "    X_res  = svd.fit_transform(X)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "    print()\n",
    "    return X_res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "Xtrvec = matutils.corpus2csc(Xtraintfidf).T.toarray()\n",
    "Xtestvec = matutils.corpus2csc(Xtesttfidf).T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 15.486722s\n",
      "Explained variance of the SVD step: 41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tr = reduce_dim_by_svd(Xtrvec, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 6.099878s\n",
      "Explained variance of the SVD step: 49%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tst = reduce_dim_by_svd(Xtestvec, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5],\n",
      " 'n_estimators': [10, 31, 52, 73, 94, 115, 136, 157, 178, 200]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 81.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [10, 31, 52, 73, 94, 115, 136, 157, 178, 200], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, None], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 50 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 115,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 40,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rf_random.cv_results_).sort_values(by='mean_test_score', ascending=False).iloc[21]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 40,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for training data: 0.97\n",
      "\n",
      "R2 for test data: -0.10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "print(\"R2 for training data: %0.2f\\n\" % best_random.score(X_tr,y_train))\n",
    "print(\"R2 for test data: %0.2f\\n\" % best_random.score(X_tst,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425.625745039463\n",
      "523.6804610576462\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_random.predict(X_tst)\n",
    "\n",
    "print(rmse(y_pred,y_test))\n",
    "\n",
    "print(mape(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "def get_dmatrix(X, y):\n",
    "    return xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "dtrain_matrix = get_dmatrix(X_tr,y_train)\n",
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "eval_metric = [\"rmse\"]\n",
    "%time model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn=10\n",
    "y_pred = np.zeros(len(testvecs))\n",
    "for i in range(len(testvecs[:1])):\n",
    "    sims = sorted(enumerate(index[vec]), key=lambda item: -item[1])[:knn]\n",
    "    y_pred[i] = np.average([y_train[k] for (k,v) in sims], weights=np.square(vec_normalize([v for (k,v) in sims])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "trainvecs = tfidf[corpus]\n",
    "trainvecs = gensim.matutils.corpus2csc(trainvecs)\n",
    "trainvecs.T.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testvecs = [tfidf[dictionary.doc2bow(doc)] for doc in docs_test]\n",
    "testvecs = gensim.matutils.corpus2csc(testvecs)\n",
    "testvecs.T.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "n_neighbors = 10\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='distance')\n",
    "y_ = knn.fit(trainvecs, y_train).predict(testvecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(enumerate(index[testvecs[0]]), key=lambda item: -item[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(vec_normalize([v for (k,v) in sorted(enumerate(index[testvecs[0]]), key=lambda item: -item[1])[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "# Define a database name (we're using a dataset on births, so we'll call it birth_db)\n",
    "# Set your postgres username/password, and connection specifics\n",
    "username = 'eli'\n",
    "password = 'elipgsql'     # change this\n",
    "host     = 'localhost'\n",
    "port     = '5432'            # default port that postgres listens on\n",
    "db_name  = 'necklaces_train'\n",
    "\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine( 'postgresql://{}:{}@{}:{}/{}'.format(username, password, host, port, db_name) )\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.DataFrame({'desc' : X_train, 'price' : y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.to_sql('necklaces_train',engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'necklaces_train'\n",
    "username = 'eli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT price FROM necklaces_train WHERE Index IN (%s) \n",
    "\"\"\" % items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to make queries using psycopg2\n",
    "con = None\n",
    "con = psycopg2.connect(database = db_name, host='/var/run/postgresql', user = username)\n",
    "\n",
    "# query:\n",
    "items = ', '.join(str(k) for k in l)\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT index, price FROM necklaces_train WHERE index IN (%s) \n",
    "\"\"\" % items\n",
    "traindata_from_sql = pd.read_sql_query(sql_query,con)\n",
    "len(traindata_from_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_from_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.square(vec_normalize([v for (k,v) in sims]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_from_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightindexer for k in traindata_from_sql['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average([traindata_from_sql.loc[traindata_from_sql['index']==k, 'price'].values[0] for (k,v) in weightindexer],weights=[v for (k,v) in weightindexer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average([traindata_from_sql[traindata_from_sql['index']==k, 'price'] for (k,v) in sims], weights=np.square(vec_normalize([v for (k,v) in sims])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightindexer = list(zip([k for (k,v) in sims],weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(traindata_from_sql['price'].values,weights=np.square(vec_normalize([v for (k,v) in sims])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn=10\n",
    "y_pred = np.zeros(len(testvecs))\n",
    "for i in range(len(testvecs[:1])):\n",
    "    sims = sorted(enumerate(index[vec]), key=lambda item: -item[1])[:knn]\n",
    "    y_pred[i] = np.average([y_train[k] for (k,v) in sims], weights=np.square(vec_normalize([v for (k,v) in sims])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [k for (k,v) in sorted(enumerate(index[trainvecs[56]]), key=lambda item: -item[1])[1:knn+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "placeholder= '?' # For SQLite. See DBAPI paramstyle.\n",
    "items = ', '.join(str(k) for k in l)\n",
    "'SELECT name FROM students WHERE id IN (%s)' % items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'gold necklace'\n",
    "doc = text.split()\n",
    "vec = tfidf[dictionary.doc2bow(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = sorted(enumerate(index[vec]), key=lambda item: -item[1])[:knn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvecs = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn = 10\n",
    "y_fit = np.zeros(len(y_train))\n",
    "trainvecs = tfidf[corpus]\n",
    "for i in range(len(trainvecs[:1])):\n",
    "    sims = sorted(enumerate(index[]), key=lambda item: -item[1])[1:knn+1]\n",
    "    print(list(sims))\n",
    "    y_fit[i] = np.average([y_train[k] for (k,v) in sims], weights=np.square(vec_normalize([v for (k,v) in sims])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[y_train[k] for (k,v) in sims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(vec_normalize([v for (k,v) in sims]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_from_sql['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_fit[~np.isnan(y_fit)]-y_train[~np.isnan(y_fit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sstot = np.sum(np.square(y_train[~np.isnan(y_fit)]-np.mean(y_train[~np.isnan(y_fit)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssresid = np.sum(np.square(resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 1-ssresid/sstot\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(resid)))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test[abs(resid)>1000], y_pred[abs(resid)>1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid[abs(resid)>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,15))\n",
    "sns.scatterplot(x=y_pred, y=resid, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.square(y_test-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean(np.sort(np.square(y_test-y_pred))[:-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[k] for (k,v) in sorted(enumerate(index[testvecs[0]]), key=lambda item: -item[1])[:5]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([y_train[k] for (k,v) in sorted(enumerate(index[testvecs[0]]), key=lambda item: -item[1])[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sorted(enumerate(index[vec]), key=lambda item: -item[1])[:5] for vec in testvecs[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[testvecs[0]]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in X_train:\n",
    "    for token in text.split():\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text.split() if frequency[token] > 1] for text in X_train]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fdist = FreqDist(word for word in ' '.join(usdf['desc'].values).split() if not word.isnumeric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.pprint(maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "usdf['adesc'] = usdf['desc'].apply(lambda desc: ' '.join([x for x in desc.split() if not bool(re.search(r'\\d', x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "descs = usdf['adesc'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf =  vectorizer.fit_transform(descs)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "denselist = tfidf.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf[usdf.price>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidffeatures = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tfidffeatures.iloc[0]\n",
    "s[s>0].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "X_reduced = TruncatedSVD(n_components=50, random_state=0).fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_embedded = TSNE(n_components=2, perplexity=40, verbose=2).fit_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(10,5))\n",
    "ax.set_xlim(0,10000)\n",
    "usdf.price.hist(ax=ax, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "norm = cm.colors.Normalize(vmax=usdf.price.values.max(), vmin=usdf.price.values.min())\n",
    "cmap = cm.jet\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(frameon=False)\n",
    "plt.setp(ax, xticks=(), yticks=())\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.9,\n",
    "                wspace=0.0, hspace=0.0)\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1],\n",
    "        c=usdf.price.values, marker=\"x\", cmap=cmap, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import  hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "X = tfidf.todense()\n",
    "threshold = 0.1\n",
    "Z = hierarchy.linkage(X,\"average\", metric=\"cosine\")\n",
    "#Clustering\n",
    "maxclust = 20\n",
    "C = hierarchy.fcluster(Z, maxclust, criterion=\"maxclust\")\n",
    "#C = hierarchy.fcluster(Z, threshold, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['clusters'] = C\n",
    "usdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_hiprice = FreqDist(word for word in ' '.join(usdf[usdf['price']>100]['adesc'].values).split() if not word.isnumeric())\n",
    "fdist_hiprice.pprint(maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,10)) \n",
    "plt.sca(ax)\n",
    "fdist_hiprice.plot(20, cumulative=False)\n",
    "fig.savefig('disthiprice.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,10)) \n",
    "plt.sca(ax)\n",
    "fdist_loprice.plot(20,cumulative=False)\n",
    "fig.savefig('distloprice.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_loprice = FreqDist(word for word in ' '.join(usdf[usdf['price']<25]['desc'].values).split() if not word.isnumeric())\n",
    "fdist_loprice.pprint(maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = usdf['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_rf = RandomForestRegressor(n_estimators=20, max_depth=None,min_samples_split=2, random_state=2, oob_score=True)\n",
    "regr_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rf = regr_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X!=tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean(np.square(y_rf-y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeby = np.zeros(len(usdf))\n",
    "eps = 1.0\n",
    "for index, row in usdf.iterrows():\n",
    "    if(index<len(usdf)-2):\n",
    "        i2 = index+1\n",
    "        row2=usdf.iloc[i2]\n",
    "        while(row['clusters']==row2['clusters']):\n",
    "            #print(row[['price','title']], row2[['price','title']])\n",
    "            if (np.abs(float(row['price'])-float(row2['price']))<eps):\n",
    "                closeby[i2]=1\n",
    "            if(i2<len(usdf)-1): \n",
    "                i2+=1\n",
    "                row2=usdf.iloc[i2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(closeby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(C)-len(np.unique(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf.groupby('clusters')['price'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets have 20 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "maxclust = 20\n",
    "C20 = hierarchy.fcluster(Z, maxclust, criterion=\"maxclust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(C20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['c20'] = C20\n",
    "usdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['overview'].apply(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'vintage' in ' '.join(['\\n    Vintage item\\n', '\\n    Favorited by: ', '\\n', '\\n        Gift wrapping and message available\\n        ', '\\n    ']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r'\\d{4}',' '.join(['\\n    Vintage item \\n', '\\n    Favorited by: ', '\\n', '\\n        Gift wrapping and message available\\n        ', '\\n    ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['vintage'] = usdf['overview'].apply(lambda x: int('vintage' in ' '.join(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['year'] = usdf['overview'].apply(lambda x: re.findall(r'\\d{4}',' '.join(x).lower())[0] if(len(re.findall(r'\\d{4}',' '.join(x).lower()))>0) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['handmade'] = usdf['overview'].apply(lambda x: int('handmade' in ' '.join(x).lower() or 'hand-made' in ' '.join(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_materials(ov):\n",
    "    l = [re.sub(r\"[^a-zA-Z\\d\\s]\", '', x).lower().replace('materials','').replace('material','').strip() for x in ov if 'materials' in x.lower() or 'material' in x.lower()] \n",
    "    return l[0] if len(l)>0 else np.nan\n",
    "    \n",
    "usdf['materials'] = usdf['overview'].apply(extract_materials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
