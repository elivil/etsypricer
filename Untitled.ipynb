{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"/tmp\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 01:12:28,922 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-01-24 01:12:28,928 : INFO : loading Dictionary object from /tmp/deerwester.dict\n",
      "2019-01-24 01:12:28,929 : INFO : loaded /tmp/deerwester.dict\n",
      "2019-01-24 01:12:28,931 : INFO : loaded corpus index from /tmp/deerwester.mm.index\n",
      "2019-01-24 01:12:28,932 : INFO : initializing cython corpus reader from /tmp/deerwester.mm\n",
      "2019-01-24 01:12:28,933 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 12 features, 28 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'deerwester.dict'))\n",
    "corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'deerwester.mm')) # comes from the first tutorial, \"Corpora and Vector Space\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 01:12:53,706 : INFO : using serial LSI version on this node\n",
      "2019-01-24 01:12:53,709 : INFO : updating model with new documents\n",
      "2019-01-24 01:12:53,710 : INFO : preparing a new chunk of documents\n",
      "2019-01-24 01:12:53,712 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-01-24 01:12:53,712 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2019-01-24 01:12:53,713 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2019-01-24 01:12:53,727 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2019-01-24 01:12:53,729 : INFO : computing the final decomposition\n",
      "2019-01-24 01:12:53,730 : INFO : keeping 2 factors (discarding 43.156% of energy spectrum)\n",
      "2019-01-24 01:12:53,731 : INFO : processed documents up to #9\n",
      "2019-01-24 01:12:53,733 : INFO : topic #0(3.341): 0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"time\" + 0.265*\"response\" + 0.240*\"computer\" + 0.221*\"human\" + 0.206*\"survey\" + 0.198*\"interface\" + 0.036*\"graph\"\n",
      "2019-01-24 01:12:53,735 : INFO : topic #1(2.542): 0.623*\"graph\" + 0.490*\"trees\" + 0.451*\"minors\" + 0.274*\"survey\" + -0.167*\"system\" + -0.141*\"eps\" + -0.113*\"human\" + 0.107*\"time\" + 0.107*\"response\" + -0.072*\"interface\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.46182100453271574), (1, -0.07002766527899992)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 01:14:11,423 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2019-01-24 01:14:11,429 : INFO : creating matrix with 9 documents and 2 features\n",
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-24 01:15:09,306 : INFO : saving MatrixSimilarity object under /tmp/deerwester.index, separately None\n",
      "2019-01-24 01:15:09,311 : INFO : saved /tmp/deerwester.index\n"
     ]
    }
   ],
   "source": [
    "index.save(os.path.join(TEMP_FOLDER, 'deerwester.index'))\n",
    "#index = similarities.MatrixSimilarity.load(os.path.join(TEMP_FOLDER, 'index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.998093), (1, 0.93748635), (2, 0.9984453), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.09879464), (8, 0.050041765)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi.save(os.path.join(TEMP_FOLDER, 'model.lsi')) # same for tfidf, lda, ...\n",
    "#lsi = models.LsiModel.load(os.path.join(TEMP_FOLDER, 'model.lsi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_memory_friendly = MyCorpus() # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import iteritems\n",
    "from smart_open import smart_open\n",
    "\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in smart_open('datasets/mycorpus.txt', 'rb'))\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a toy corpus of 2 documents, as a plain Python list\n",
    "corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it\n",
    "\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.mm'), corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'corpus.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "numpy_matrix = np.random.randint(10, size=[5,2])\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "numpy_matrix_dense = gensim.matutils.corpus2dense(corpus, num_terms=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_matrix_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
