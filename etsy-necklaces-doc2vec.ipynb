{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('etsynecklaces10000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9967"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf=df[df['cur']=='USD']\n",
    "len(usdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "usdf['price'] = pd.to_numeric(usdf['price'].apply(lambda x: x.replace(',','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "usdf['description'] = usdf['description'].apply(lambda x: ''.join(x).strip())\n",
    "linkregex = r\"((([A-Za-z]{3,9}:(?:\\/\\/)?)(?:[-;:&=\\+\\$,\\w]+@)?[A-Za-z0-9.-]+|(?:www.|[-;:&=\\+\\$,\\w]+@)[A-Za-z0-9.-]+)((?:\\/[\\+~%\\/.\\w\\-_]*)?\\??(?:[-\\+=&;%@.\\w_]*)#?(?:[\\w]*))?)\"\n",
    "usdf['description'] = usdf['description'].str.replace(linkregex, '')\n",
    "usdf['description'] = usdf['description'].str.replace(r\"[^a-zA-Z\\d\\s]\", '')\n",
    "usdf['description'] = usdf['description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "#usdf['desc'] = usdf['description'].apply(lambda desc: ' '.join([stemmer.stem(x) for x in desc.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "s=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "usdf['desc'] = usdf['description'].apply(lambda desc: ' '.join([x for x in desc.split() if not x in s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "usdf['adesc'] = usdf['desc'].apply(lambda desc: ' '.join([x for x in desc.split() if not bool(re.search(r'\\d', x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       beautiful highquality italian chains created f...\n",
       "1       name necklace silver name necklace personalize...\n",
       "2       sale sale special offer spend shop receive dis...\n",
       "3       moonstone mala necklace moonstone necklace moo...\n",
       "4       original filigree trancelim double necklace fa...\n",
       "5       custom made name necklace stainless steel gold...\n",
       "6       engraved silver initial charm fire enamel coup...\n",
       "7       personalized necklacedainty necklacename neckl...\n",
       "8       cross necklace men mens gold cross necklace me...\n",
       "9       embrace magic moon made order rainbow moonston...\n",
       "10      sterling silver personalized volleyball sport ...\n",
       "12      mens necklace mens jewelry triangle pendant st...\n",
       "13      give special personalized message year always ...\n",
       "14      trendy geometric necklace necklace features be...\n",
       "15      valentines gift personalized locket necklace i...\n",
       "17      free gift wrapping jewelry comes beautiful gif...\n",
       "18      hedgehog x length open necklace laid straight ...\n",
       "20      delicate sweet beavailable gold silver rose go...\n",
       "21      lovelyl cute necklacethis simple necklace made...\n",
       "22      sale save purchase code ohsosweetsweet birthda...\n",
       "23      beleco jewelry making beautiful jewelry woman ...\n",
       "24      dainty gold double horn necklace crescent moon...\n",
       "25      simple citrine sterling silver layering neckla...\n",
       "26      soulmate personalized initial couple necklace ...\n",
       "27      sisters necklacerose gold orchid flower disc n...\n",
       "28      detash clean accumulated crafting supplies pas...\n",
       "29      sale save purchase code ohsosweettell us engra...\n",
       "30      dainty necklaces chokers earrings waiting desi...\n",
       "31      paw patrol skye chunky necklace braceletpaw pa...\n",
       "32      rose necklace rose gold initial necklace rose ...\n",
       "                              ...                        \n",
       "9924    london blue topaz necklace made genuine london...\n",
       "9926    sweet open heart loved hand stamped silver str...\n",
       "9928    labradorite necklace absoluterly gorgeous high...\n",
       "9929    jorma laines jewelry designs pure organic form...\n",
       "9932    stunning quartz crystal encased copper copper ...\n",
       "9933    simple backdrop necklace bridal back drop neck...\n",
       "9934    elegant wedding necklace made swarovski crysta...\n",
       "9937    family medallion necklace gold sterling smallt...\n",
       "9938    raw peridot necklace crystal necklace girlfrie...\n",
       "9940    sterling silver big small dandelion charm neck...\n",
       "9941    beautiful laboradite necklace larger faceted s...\n",
       "9943    british crown minted many years one issued fea...\n",
       "9944    wonderful selro blackamoor asian princess neck...\n",
       "9945    cute little reminder favorite place person cho...\n",
       "9946    phoenix bird mythological bird associated revi...\n",
       "9947    made ordera playful funky necklace animal love...\n",
       "9948    regal coat arms dangles chain pendant measures...\n",
       "9950    offering designs solid gold inquire pricing ev...\n",
       "9951    necklace delicate strong handcrochet genuine b...\n",
       "9952    best worldshammered smooth togetherthree sterl...\n",
       "9955    stainless steel cross abstract cross art uniqu...\n",
       "9957    customize personal jewelry craftsmanyou press ...\n",
       "9958    simple silver jerusalem cross pendant makes si...\n",
       "9960    capricorn zodiac constellation necklace made s...\n",
       "9961    chelsea bracelet sold separate see link belowi...\n",
       "9962    charm created using image fingerprint mold fin...\n",
       "9963    personalize jewelry please check item details ...\n",
       "9964    solid yellow gold foxtail chain necklace inch ...\n",
       "9965    revenge heart pendant sterling silver solid go...\n",
       "9966    larimar moonstone kunzite amazonite mala beads...\n",
       "Name: adesc, Length: 8257, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf['adesc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def viterbi_segment(text):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        #(log(1/total)-max_word_len-1)*(j-i) \n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "                        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words, probs[-1]\n",
    "\n",
    "def word_prob(word): return dictionary_US[word] / total\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "dictionary_US = Counter(words(open('/home/eli/Downloads/big.txt').read()))\n",
    "max_word_length = max(map(len, dictionary_US))\n",
    "total = float(sum(dictionary_US.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eli/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/usr/share/dict/american-english') as fdict:\n",
    "    US_dict = fdict.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "US_dict = [x.strip() for x in US_dict] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'highquality' in US_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42min 59s, sys: 1.76 s, total: 43min 1s\n",
      "Wall time: 1h 57min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/anaconda3/envs/etsyapp/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "usdf['adesc'] = usdf['adesc'].apply(lambda x: ' '.join(flatten([[word for word in viterbi_segment(token)[0]] for token in x.split() if not token in US_dict])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       beautiful high quality italian chains created ...\n",
       "1       name necklace silver name necklace personal i ...\n",
       "2       sale sale special offer spend shop receive dis...\n",
       "3       moon stone mala necklace moon stone necklace m...\n",
       "4       original fili g re e trance li m double neckla...\n",
       "5       custom made name necklace stain less steel gol...\n",
       "6       engraved silver initial charm fire enamel coup...\n",
       "7       personal i ze d necklace dainty necklace name ...\n",
       "8       cross necklace men men s gold cross necklace m...\n",
       "9       embrace magic moon made order rainbow moon sto...\n",
       "10      sterling silver personal i ze d volley ball sp...\n",
       "12      men s necklace men s jewel r y triangle p end ...\n",
       "13      give special personal i ze d message year alwa...\n",
       "14      trend y geometric necklace necklace features b...\n",
       "15      valentine s gift personal i ze d locket neckla...\n",
       "17      free gift wrapping jewel r y comes beautiful g...\n",
       "18      hedge ho g x length open necklace laid straigh...\n",
       "20      delicate sweet be available gold silver rose g...\n",
       "21      lovely l cut e necklace this simple necklace m...\n",
       "22      sale save purchase code oh so sweet sweet birt...\n",
       "23      be le co jewel r y making beautiful jewel r y ...\n",
       "24      dainty gold double horn necklace crescent moon...\n",
       "25      simple c it r in e sterling silver layer ing n...\n",
       "26      soul mate personal i ze d initial couple neckl...\n",
       "27      sisters necklace rose gold or c hid flower dis...\n",
       "28      de t ash clean accumulated craft ing supplies ...\n",
       "29      sale save purchase code oh so sweet tell us en...\n",
       "30      dainty necklaces choke r s earrings waiting de...\n",
       "31      paw patrol sky e ch un k y necklace brace let ...\n",
       "32      rose necklace rose gold initial necklace rose ...\n",
       "                              ...                        \n",
       "9924    london blue top a z necklace made genuine lond...\n",
       "9926    sweet open heart loved hand stamped silver st ...\n",
       "9928    la br a do rite necklace absolute r ly gorgeou...\n",
       "9929    j or ma lain es jewel r y designs pure organic...\n",
       "9932    stunning quart z crystal encased copper copper...\n",
       "9933    simple back drop necklace bridal back drop nec...\n",
       "9934    elegant wedding necklace made s war o v ski cr...\n",
       "9937    family medal lion necklace gold sterling small...\n",
       "9938    raw peri dot necklace crystal necklace girlfri...\n",
       "9940    sterling silver big small d and e lion charm n...\n",
       "9941    beautiful labor a d it e necklace larger facet...\n",
       "9943    british crown mint ed many years one issued fe...\n",
       "9944    wonderful s el r o black a moor as i an prince...\n",
       "9945    cut e little reminder favorite place person ch...\n",
       "9946    phoenix bird mythological bird associated revi...\n",
       "9947    made order a playful fun k y necklace animal l...\n",
       "9948    regal coat arms dangles chain p end an t measu...\n",
       "9950    offering designs solid gold inquire p ri c ing...\n",
       "9951    necklace delicate strong hand c r o c he t gen...\n",
       "9952    best worlds hammered smooth together three ste...\n",
       "9955    stain less steel cross abstract cross art uniq...\n",
       "9957    custom i ze personal jewel r y craftsman you p...\n",
       "9958    simple silver jerusalem cross p end an t makes...\n",
       "9960    cap ri corn z o di a c con s tell at i on neck...\n",
       "9961    ch else a brace let sold separate see link bel...\n",
       "9962    charm created using image finger print mold fi...\n",
       "9963    personal i ze jewel r y please check item deta...\n",
       "9964    solid yellow gold fox tail chain necklace inch...\n",
       "9965    revenge heart p end an t sterling silver solid...\n",
       "9966    la rim a r moon stone k un z it e amazon it e ...\n",
       "Name: adesc, Length: 8257, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf['adesc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, tuple found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-92e56c31bfa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mviterbi_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adesc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, tuple found"
     ]
    }
   ],
   "source": [
    "' '.join([viterbi_segment(token) for token in usdf['adesc'][0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    usdf['adesc'].values, usdf['price'].values, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = \"/home/eli/code/insight/etsy/etsyitems/nlp\"\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text.split() for text in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = [text.split() for text in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC2VEC ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def read_corpus(texts, tokens_only=False):\n",
    "    for i, text in enumerate(texts):\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(' '.join(text))\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(' '.join(text)), [i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(texts))\n",
    "test_corpus = list(read_corpus(docs_test, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "collections.Counter(ranks)  # Results vary between runs due to random seeding and very small corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(usdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(usdf.drop_duplicates(subset=['adesc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = np.random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inferred_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(TEMP_FOLDER, 'necklaces.d2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer_vector(train_corpus[0].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X_tr = np.array([model.infer_vector(train_corpus[i].words, steps=40, alpha=0.025) for i in range(len(train_corpus))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X_tst = np.array([model.infer_vector(test_corpus[i], steps=40, alpha=0.025) for i in range(len(test_corpus))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/usr/share/dict/american-english') as US_dict:\n",
    "    content = US_dict.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def viterbi_segment(text):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        #(log(1/total)-max_word_len-1)*(j-i) \n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "                        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words, probs[-1]\n",
    "\n",
    "def word_prob(word): return dictionary_US[word] / total\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "dictionary_US = Counter(words(open('/home/eli/Downloads/big.txt').read()))\n",
    "max_word_length = max(map(len, dictionary_US))\n",
    "total = float(sum(dictionary_US.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ' '.join(viterbi_segment('skillssafe')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 100, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "#max_depth = [int(x) for x in np.linspace(10, 100, num = 5)]\n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "#bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf\n",
    "              }\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_tst, y_test):\n",
    "    predictions = model.predict(X_tst)\n",
    "    resid = predictions - y_test\n",
    "    print(predictions[abs(resid)>500],y_test[abs(resid)>500])\n",
    "    errors = abs(resid)\n",
    "    mape = 100 * np.mean(errors / y_test)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} dollars.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    fig,ax = plt.subplots(figsize=(10,15))\n",
    "    sns.scatterplot(x=y_test, y=errors, ax=ax)\n",
    "    \n",
    "    return accuracy, resid\n",
    "\n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_tr, y_train)\n",
    "base_accuracy, resid = evaluate(base_model, X_tst, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[' '.join(test_corpus[i]) for i in range(len(test_corpus)) if abs(resid[i])>500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,15))\n",
    "sns.scatterplot(x=predictions, y=errors, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_tst, y_test)\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'necklaces.dict'))  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'necklaces.mm'), corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "tfidf.save(os.path.join(TEMP_FOLDER, 'necklacesmodel.tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus]) # transform corpus to Tfidf space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save(os.path.join(TEMP_FOLDER, 'necklacestfidfsim.index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = [text.split() for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testvecs = [tfidf[dictionary.doc2bow(doc)] for doc in docs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_normalize(vec):\n",
    "    return vec/np.sqrt(np.sum(np.square(vec)))\n",
    "\n",
    "assert(np.array_equal(vec_normalize([1,0]),[1,0]))\n",
    "#vec_normalize([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(enumerate(index[testvecs[0]]), key=lambda item: -item[1])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(vec_normalize([v for (k,v) in sorted(enumerate(index[testvecs[0]]), key=lambda item: -item[1])[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "# Define a database name (we're using a dataset on births, so we'll call it birth_db)\n",
    "# Set your postgres username/password, and connection specifics\n",
    "username = 'eli'\n",
    "password = 'elipgsql'     # change this\n",
    "host     = 'localhost'\n",
    "port     = '5432'            # default port that postgres listens on\n",
    "db_name  = 'necklaces_train'\n",
    "\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine( 'postgresql://{}:{}@{}:{}/{}'.format(username, password, host, port, db_name) )\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.DataFrame({'desc' : X_train, 'price' : y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.to_sql('necklaces_train',engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'necklaces_train'\n",
    "username = 'eli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT price FROM necklaces_train WHERE Index IN (%s) \n",
    "\"\"\" % items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to make queries using psycopg2\n",
    "con = None\n",
    "con = psycopg2.connect(database = db_name, host='/var/run/postgresql', user = username)\n",
    "\n",
    "# query:\n",
    "items = ', '.join(str(k) for k in l)\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT index, price FROM necklaces_train WHERE index IN (%s) \n",
    "\"\"\" % items\n",
    "traindata_from_sql = pd.read_sql_query(sql_query,con)\n",
    "len(traindata_from_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_from_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.square(vec_normalize([v for (k,v) in sims]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_from_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightindexer for k in traindata_from_sql['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average([traindata_from_sql.loc[traindata_from_sql['index']==k, 'price'].values[0] for (k,v) in weightindexer],weights=[v for (k,v) in weightindexer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average([traindata_from_sql[traindata_from_sql['index']==k, 'price'] for (k,v) in sims], weights=np.square(vec_normalize([v for (k,v) in sims])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightindexer = list(zip([k for (k,v) in sims],weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(traindata_from_sql['price'].values,weights=np.square(vec_normalize([v for (k,v) in sims])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn=10\n",
    "y_pred = np.zeros(len(testvecs))\n",
    "for i in range(len(testvecs[:1])):\n",
    "    sims = sorted(enumerate(index[vec]), key=lambda item: -item[1])[:knn]\n",
    "    y_pred[i] = np.average([y_train[k] for (k,v) in sims], weights=np.square(vec_normalize([v for (k,v) in sims])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [k for (k,v) in sorted(enumerate(index[trainvecs[56]]), key=lambda item: -item[1])[1:knn+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "placeholder= '?' # For SQLite. See DBAPI paramstyle.\n",
    "items = ', '.join(str(k) for k in l)\n",
    "'SELECT name FROM students WHERE id IN (%s)' % items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'gold necklace'\n",
    "doc = text.split()\n",
    "vec = tfidf[dictionary.doc2bow(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = sorted(enumerate(index[vec]), key=lambda item: -item[1])[:knn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvecs = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn = 10\n",
    "y_fit = np.zeros(len(y_train))\n",
    "trainvecs = tfidf[corpus]\n",
    "for i in range(len(trainvecs[:1])):\n",
    "    sims = sorted(enumerate(index[]), key=lambda item: -item[1])[1:knn+1]\n",
    "    print(list(sims))\n",
    "    y_fit[i] = np.average([y_train[k] for (k,v) in sims], weights=np.square(vec_normalize([v for (k,v) in sims])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[y_train[k] for (k,v) in sims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(vec_normalize([v for (k,v) in sims]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_from_sql['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y_fit[~np.isnan(y_fit)]-y_train[~np.isnan(y_fit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sstot = np.sum(np.square(y_train[~np.isnan(y_fit)]-np.mean(y_train[~np.isnan(y_fit)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssresid = np.sum(np.square(resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 1-ssresid/sstot\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(resid)))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test[abs(resid)>1000], y_pred[abs(resid)>1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid[abs(resid)>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,15))\n",
    "sns.scatterplot(x=y_pred, y=resid, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(np.square(y_test-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean(np.sort(np.square(y_test-y_pred))[:-100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[k] for (k,v) in sorted(enumerate(index[testvecs[0]]), key=lambda item: -item[1])[:5]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([y_train[k] for (k,v) in sorted(enumerate(index[testvecs[0]]), key=lambda item: -item[1])[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sorted(enumerate(index[vec]), key=lambda item: -item[1])[:5] for vec in testvecs[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[testvecs[0]]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in X_train:\n",
    "    for token in text.split():\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text.split() if frequency[token] > 1] for text in X_train]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fdist = FreqDist(word for word in ' '.join(usdf['desc'].values).split() if not word.isnumeric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.pprint(maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "usdf['adesc'] = usdf['desc'].apply(lambda desc: ' '.join([x for x in desc.split() if not bool(re.search(r'\\d', x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "descs = usdf['adesc'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf =  vectorizer.fit_transform(descs)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "denselist = tfidf.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf[usdf.price>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidffeatures = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tfidffeatures.iloc[0]\n",
    "s[s>0].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "X_reduced = TruncatedSVD(n_components=50, random_state=0).fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_embedded = TSNE(n_components=2, perplexity=40, verbose=2).fit_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(10,5))\n",
    "ax.set_xlim(0,10000)\n",
    "usdf.price.hist(ax=ax, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "norm = cm.colors.Normalize(vmax=usdf.price.values.max(), vmin=usdf.price.values.min())\n",
    "cmap = cm.jet\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(frameon=False)\n",
    "plt.setp(ax, xticks=(), yticks=())\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.9,\n",
    "                wspace=0.0, hspace=0.0)\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1],\n",
    "        c=usdf.price.values, marker=\"x\", cmap=cmap, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import  hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "X = tfidf.todense()\n",
    "threshold = 0.1\n",
    "Z = hierarchy.linkage(X,\"average\", metric=\"cosine\")\n",
    "#Clustering\n",
    "maxclust = 20\n",
    "C = hierarchy.fcluster(Z, maxclust, criterion=\"maxclust\")\n",
    "#C = hierarchy.fcluster(Z, threshold, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['clusters'] = C\n",
    "usdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_hiprice = FreqDist(word for word in ' '.join(usdf[usdf['price']>100]['adesc'].values).split() if not word.isnumeric())\n",
    "fdist_hiprice.pprint(maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,10)) \n",
    "plt.sca(ax)\n",
    "fdist_hiprice.plot(20, cumulative=False)\n",
    "fig.savefig('disthiprice.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,10)) \n",
    "plt.sca(ax)\n",
    "fdist_loprice.plot(20,cumulative=False)\n",
    "fig.savefig('distloprice.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_loprice = FreqDist(word for word in ' '.join(usdf[usdf['price']<25]['desc'].values).split() if not word.isnumeric())\n",
    "fdist_loprice.pprint(maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = usdf['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_rf = RandomForestRegressor(n_estimators=20, max_depth=None,min_samples_split=2, random_state=2, oob_score=True)\n",
    "regr_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rf = regr_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X!=tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean(np.square(y_rf-y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeby = np.zeros(len(usdf))\n",
    "eps = 1.0\n",
    "for index, row in usdf.iterrows():\n",
    "    if(index<len(usdf)-2):\n",
    "        i2 = index+1\n",
    "        row2=usdf.iloc[i2]\n",
    "        while(row['clusters']==row2['clusters']):\n",
    "            #print(row[['price','title']], row2[['price','title']])\n",
    "            if (np.abs(float(row['price'])-float(row2['price']))<eps):\n",
    "                closeby[i2]=1\n",
    "            if(i2<len(usdf)-1): \n",
    "                i2+=1\n",
    "                row2=usdf.iloc[i2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(closeby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(C)-len(np.unique(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf.groupby('clusters')['price'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets have 20 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "maxclust = 20\n",
    "C20 = hierarchy.fcluster(Z, maxclust, criterion=\"maxclust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(C20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['c20'] = C20\n",
    "usdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['overview'].apply(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'vintage' in ' '.join(['\\n    Vintage item\\n', '\\n    Favorited by: ', '\\n', '\\n        Gift wrapping and message available\\n        ', '\\n    ']).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r'\\d{4}',' '.join(['\\n    Vintage item \\n', '\\n    Favorited by: ', '\\n', '\\n        Gift wrapping and message available\\n        ', '\\n    ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['vintage'] = usdf['overview'].apply(lambda x: int('vintage' in ' '.join(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['year'] = usdf['overview'].apply(lambda x: re.findall(r'\\d{4}',' '.join(x).lower())[0] if(len(re.findall(r'\\d{4}',' '.join(x).lower()))>0) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf['handmade'] = usdf['overview'].apply(lambda x: int('handmade' in ' '.join(x).lower() or 'hand-made' in ' '.join(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_materials(ov):\n",
    "    l = [re.sub(r\"[^a-zA-Z\\d\\s]\", '', x).lower().replace('materials','').replace('material','').strip() for x in ov if 'materials' in x.lower() or 'material' in x.lower()] \n",
    "    return l[0] if len(l)>0 else np.nan\n",
    "    \n",
    "usdf['materials'] = usdf['overview'].apply(extract_materials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
